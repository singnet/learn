Fake (Artificial) Languages
===========================

Tools to create custom-taylored fake languages having custom-tailored
statistical properties. This will allow the learning code to be
evaluated on the corpora generated by these languages, thus allowing
a better understanding of how the learning code is able to extract
structure from language.

The advantages of doing this:

* The ability to strictly control the size of the vocabulary.
  (This is much easier than curating a corpus of child-directed
  writing and speech, or a corpus of simple English (Simple English
  Wikipedia; Thing Explainer).

* The ability to control the grammar of the language. This includes:

  * Controlling the arity (number of dependents) that a word may have
    (e.g. transitive verbs have an arity of two: a connector to the
    subject, and a connector to the object. Common nouns have an arity
    of one: a connector to the verb.)

  * Controlling the relative frequency of nouns, verbs, modifiers.

  * Controlling the number of word-senses that a word might participate
    in, thus validating word-sense disambiguation abilities.

* The ability to control the frequency of word distributions. In natural
  language, word distributions tend to be Zipfian, and thus have a
  vocabulary dependent on the corpus size. In Hanzi languages, the
  number of Hanzi is relatively fixed, and one can potentially
  over-sample. Using an artificial languge can help disentangle such
  effects.

* The ability to generate a perfect corpus, free of stray markup
  typically found in natural corpora. Natural corpora tend to have
  typesetting markup (e.g. html) that leaks through, no matter how
  much scrubbing is applied. Natural corpora also tend to have
  tables, indexes and lists, which are not grammatically structured
  or are only weakly structured. By using a perfect corpus, these
  can be eliminated, or they can be introduced in known ways.

* Most importantly: the ability to perform a perfect evaluation of
  the learning results, since the grammar is known exactly. The
  measurement of accuracy, precision, and recall can be measured
  precisely, without any need for a "golden reference" or
  linguist-generated reference parses.

Status
------
Project just started. Nothing here yet.

See the [Network Generation project](https://github.com/opencog/generate/)
for the code that will be used to generate a corpus of "grammatically
valid sentences" aka syntactic trees from a randomly generated grammar.
